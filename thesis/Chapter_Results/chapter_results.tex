\section{Results}\label{sec:results}
In this thesis, the \glsfirst{cfd} application RAFSINE which implemented the \glsfirst{lbm} was deployed on a remote \gls{gpu} enabled server at \gls{rsn}. The \gls{vgl} toolkit was configured on the server to access the \gls{opengl} based visualization and user interface over a local network through the remote access system \gls{vnc}. 

\gls{vgl} introduced a slight performance penalty from its in-process \gls{glx} forking, which was improved by modifying the application to use multi-threading. This decoupled the \gls{opengl} based visualization from the execution of the \gls{cfd} simulation \gls{cuda} kernel and allowed the kernel to utilize a larger amount of the available computational resources.

The \gls{lua}-to-\gls{cuda} code generation \gls{api} built into RAFSINE was used to create a simulation model of the experimental data center POD 2 at \gls{rsn}. In order to simulate transient behavior of server power usage, the \gls{cuda} simulation kernel was modified to allow real-time updates of boundary conditions while the simulation was running. Code was also added to calculate lattice array indices of positions adjacent to the boundary conditions, to sample temperatures and volumetric flow rates during simulations.

In order to validate the model with experimental measurements, the simulation was set to update its boundary conditions representing server power usage and \gls{crac} air conditioning at the same rate as the experiment. On a lattice resolution of 36 sites per meter, the model was found to be accurate within $\pm 1$\degree C on a room level when measured as the average error of inlet return air to the \gls{crac}s, with a maximum error of $\pm 2$\degree C.

When comparing the temperatures on a server rack level using the sensor strips on the front of the racks, an error of approximately $\pm 1$\degree C, $\pm 2$\degree C and $\pm 4$\degree C was observed for the bottom, middle and top sensors respectively. The temperatures on the back of the racks were on average accurate within $\pm 6$\degree C. The error on the back temperature sensors, and to an extent the front sensors was expected because of lack of air flow data for some of the servers, as well as simplifications made when modeling the boundary conditions.

\section{Conclusion}\label{sec:conclusion}
As mentioned in the introduction of this thesis, one of the main use cases for this type of simulation is for testing different physical configurations and placements of heat generating servers and their air cooling units. While further refinement of the model is needed to get the error of the simulated temperatures closer to the experimental error, different placements of equipment could be quite easily tested by adjusting the coordinates of their boundary conditions in the \gls{lua} code generation script (mentioned in chapter \ref{sec:lbm_impl}). 

%However, this is quite a cumbersome process, and the integration of a graphical Computer-Aided Design (CAD) interface should be a high priority for improving this use case.

Another situation in which the simulation could be used is in testing air conditioning control systems. While the model created in this project set the boundary conditions for temperatures and air flows based on recorded data for the purposes of model validation, it would be possible to set them based on the output of an external control system program. This program could theoretically use the simulated temperature sensors as control inputs, as long as the required rate of measurements is lower than the rate of simulated constant time steps (see chapter \ref{sec:dlb}).

The final use case this project set out to investigate was for producing data sets for use in sensor based automatic fault detection systems built on machine learning. Although no actual testing of such systems was done in this project, the program could easily be adjusted to produce machine learning data sets of temperatures and air flows in situations where some of the \gls{crac}s had malfunctioned. This situation would simply be a matter of setting the boundary conditions representing the \gls{crac}s to produce no air flow through their heat exchange inlets and exhausts. The record of the simulated temperatures could then be used as an input data set, from which the algorithm could learn to detect the signs of malfunctions from sensor values.

Finally, it should be said that while the \gls{lbm} algorithm is not the only solution for \gls{cfd} simulations of the thermal flow in data centers, its capacity to produce predictions in real-time or faster allows it to be used directly in automated monitoring and control systems. Its fast convergence could allow it to update its thermal model directly from sensor values in the real world data center, predict future conditions faster than real time, and set real world control signals accordingly. However, it should also be said that a complete \gls{cfd} simulation of thermal conditions might not be necessary in the case of control systems. Simpler models which require lower amounts of computational resources might suffice.

\section{Future Work}
During initial experiments with simulating the data center model, a higher resolution grid was found to improve the accuracy of the predictive simulation. Because of time constraints, the effect of lattice resolution on accuracy was not investigated fully, since a larger lattice significantly reduced the rate at which the simulation could be performed relative to real time. In his thesis, the RAFSINE author Nicolas Delbosc investigated executing the simulation on two different \gls{gpu}s cooperating using the NVIDIA Peer-to-Peer framework and found that the simulation performed only 3.6\% slower than twice as fast~\cites[pg.100]{Delbosc}. While hardware limitations in the form of maximum PCI-Express bandwidth might limit the performance gain, further investigation in this area might allow for higher resolution lattices and larger domains when multiple \gls{gpu}s can cooperate to execute the simulation.

Another improvement Delbosc suggested in his thesis~\cites[pg.195]{Delbosc} is the development of a better graphical user interface for geometry construction and parameter modification at runtime. While the work done during this master thesis did implement the latter, a noticable performance decrease was added because of the way it was implemented. This code change for modifying boundary conditions during runtime could use further optimization. 

The \gls{lua}-to-\gls{cuda} code generation \gls{api} was used for geometry construction, but the integration of a 3D graphics toolkit such as OpenSceneGraph\footnote{\url{https://www.openscenegraph.org/}} could allow for the creation of a type of CAD interface like the ones seen in commercial \gls{cfd} packages such as ANSYS\footnote{\url{https://www.ansys.com/}} . The interface could also allow the user to set custom boundary conditions functions using code generation techniques similar to the existing ones.

Finally, in this thesis work the \gls{bgk} simulation model was used for validation but, as Delbosc mentioned in his thesis, there exist other models such as Multiple Relaxation Time (MRT) and Cascaded \gls{lbm}. These models can provide a higher degree of numerical stability for simulating turbulent fluid flow \gls{bgk}~\cites[pg.196]{Delbosc}. Implementing these models would require knowledge of fluid dynamics, \gls{lbm}, the existing RAFSINE application and the programming techniques and languages used to create it.

